{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(max_retries=5, api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\", **kwargs) -> List[float]:\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    response = client.embeddings.create(input=[text], model=model, **kwargs)\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings from dataset\n",
    "\n",
    "This notebook gives an example on how to get embeddings from a large dataset.\n",
    "\n",
    "\n",
    "## 1. Load the dataset\n",
    "\n",
    "The dataset used in this example is [fine-food reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews) from Amazon. The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of this dataset, consisting of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).\n",
    "\n",
    "We will combine the review summary and review text into a single combined text. The model will encode this combined text and it will output a single vector embedding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, you will need to install: pandas, openai, transformers, plotly, matplotlib, scikit-learn, torch (transformer dep), torchvision, and scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8000  # the maximum for text-embedding-3-small is 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To add an index column\n",
    "# # load & inspect dataset\n",
    "# input_datapath = \"Data/Product_Information_Dataset.csv\"  # to save space, we provide a pre-filtered dataset\n",
    "# df = pd.read_csv(input_datapath)\n",
    "# df.reset_index(inplace=True)\n",
    "# df.to_csv(\"Data/Product_Information_Dataset_with_index.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_12908\\1552472914.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'None' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"None\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# # To replace empty values with \"\"\n",
    "# # load & inspect dataset\n",
    "# input_datapath = \"Data/Product_Information_Dataset_with_index.csv\"  # to save space, we provide a pre-filtered dataset\n",
    "# df = pd.read_csv(input_datapath)\n",
    "# df.fillna(\"None\", inplace=True)\n",
    "# df.to_csv(\"Data/Product_Information_Dataset_with_index_and_no_empty_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load & inspect dataset\n",
    "input_datapath = \"Data/Product_Information_Dataset.csv\"  # to save space, we provide a pre-filtered dataset\n",
    "df = pd.read_csv(input_datapath)\n",
    "# only include these columns, which is all of the columns\n",
    "df = df[\n",
    "    [\n",
    "        \"main_category\",\n",
    "        \"title\",\n",
    "        \"average_rating\",\n",
    "        \"rating_number\",\n",
    "        \"features\",\n",
    "        \"description\",\n",
    "        \"price\",\n",
    "        \"store\",\n",
    "        \"categories\",\n",
    "        \"details\",\n",
    "        \"parent_asin\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Ensure all columns are strings before concatenation so it doesn't throw an error\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"description\"] = df[\"description\"].astype(str)\n",
    "df[\"features\"] = df[\"features\"].astype(str)\n",
    "df[\"details\"] = df[\"details\"].astype(str)\n",
    "df[\"categories\"] = df[\"categories\"].astype(str)\n",
    "df[\"store\"] = df[\"store\"].astype(str)\n",
    "df[\"main_category\"] = df[\"main_category\"].astype(str)\n",
    "\n",
    "# df = df.dropna()\n",
    "# TODO experiment with less columns included\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \"\n",
    "    + df.title.str.strip()\n",
    "    + \"; Description: \"\n",
    "    + df.description.str.strip()\n",
    "    + \"; Features: \"\n",
    "    + df.features.str.strip()\n",
    "    + \"; Details: \"\n",
    "    + df.details.str.strip()\n",
    "    + \"; Categories: \"\n",
    "    + df.categories.str.strip()\n",
    "    + \"; Store: \"\n",
    "    + df.store.str.strip()\n",
    "    + \"; Main Category: \"\n",
    "    + df.main_category.str.strip()\n",
    ")\n",
    "df.head(2)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 100000000000\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# omit reviews that are too long to embed\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get embeddings and save them for future reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n",
    "\n",
    "# This may take a few minutes\n",
    "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, model=embedding_model))\n",
    "df.to_csv(\"Data/Product_Information_Dataset_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_embedding(\"BOYA BYM1\", model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic search using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def download_file_from_s3(bucket_name, object_name, file_name):\n",
    "  s3.download_file(bucket_name, object_name, file_name)\n",
    "\n",
    "bucket_name = \"chatbot-store-genailabs\"\n",
    "object_name = \"Product_Information_Dataset_with_embeddings.csv\"\n",
    "file_name = \"Data/Product_Information_Dataset_with_embeddings.csv\" # local file name\n",
    "\n",
    "download_file_from_s3(bucket_name, object_name, file_name)\n",
    "\n",
    "datafile_path = \"Data/Product_Information_Dataset_with_embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search through the reviews for a specific product\n",
    "def search_embeddings(df, query, n=3, pprint=True):\n",
    "    embedding = get_embedding(query, model=\"text-embedding-3-small\")\n",
    "    df[\"similarities\"] = df.embedding.apply(\n",
    "        lambda x: cosine_similarity(x, embedding)\n",
    "    )\n",
    "    res = df.sort_values(\"similarities\", ascending=False).head(n)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_embeddings(df, \"BOYA BYM1 Microphone\", n=10000)\n",
    "res = res.drop(columns=[\"combined\", \"embedding\"])\n",
    "res.to_csv(\"search_results_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, instead of embeddings in the csv, we could use Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the index. Don't run this cell if you want to use the existing index.\n",
    "from pinecone import ServerlessSpec,Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"chatbot-store\"  # put in the name of your pinecone index here. When creating the index in pinecone.io, the Dimensions have to be the same as the result length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To empty the index. DONT RUN THIS CELL IF YOU WANT TO KEEP THE INDEX\n",
    "# if index_name in pc.list_indexes().names():\n",
    "#     pc.delete_index(index_name)\n",
    "\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=1536,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "# )\n",
    "# while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "#     time.sleep(1)\n",
    "\n",
    "# index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we create a vector embedding for each question using OpenAI (as demonstrated earlier), and upsert the ID, vector embedding, and original text for each phrase to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "count = 0  # we'll use the count to create unique IDs\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(trec[\"text\"]), batch_size)):\n",
    "    # set end position of batch\n",
    "    i_end = min(i + batch_size, len(trec[\"text\"]))\n",
    "    # get batch of lines and IDs\n",
    "    lines_batch = trec[\"text\"][i : i + batch_size]\n",
    "    ids_batch = [str(n) for n in range(i, i_end)]\n",
    "    # create embeddings\n",
    "    res = client.embeddings.create(input=lines_batch, model=MODEL)\n",
    "    embeds = [record.embedding for record in res.data]\n",
    "    # prep metadata and upsert batch\n",
    "    meta = [{\"text\": line} for line in lines_batch]\n",
    "    to_upsert = zip(ids_batch, embeds, meta)\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=list(to_upsert))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
